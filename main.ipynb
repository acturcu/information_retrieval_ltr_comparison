{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python 3.10 for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install \"numpy<2.0\" python-terrier==0.12.1 nltk scikit-learn lightgbm fastrank tensorflow==2.11 keras LambdaRankNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets so they are ready to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_setups import NFCorpus, Antique\n",
    "\n",
    "datasets = {\n",
    "    \"nfcorpus\": NFCorpus(),\n",
    "    \"antique\": Antique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[\"antique\"]  # Change to \"antique\" for the Antique dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = pt.terrier.FeaturesRetriever(\n",
    "    dataset.index,\n",
    "    wmodel=\"BM25\",\n",
    "    features=[\"WMODEL:BM25\", \"WMODEL:PL2\", \"WMODEL:DPH\"],\n",
    "    num_results=100,\n",
    "    metadata=dataset.metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lambdaMART import *\n",
    "from models.lambdaRank import *\n",
    "from models.rankSVM import *\n",
    "from models.coordAscent import *\n",
    "from models.randomForest import *\n",
    "from models.rankNet import *\n",
    "import datetime\n",
    "\n",
    "models = [\n",
    "    (\"lambdaMART\", get_lambdaMART_model(base_model)),\n",
    "    (\"lambdaRank\", get_lambdaRank_model(base_model)),\n",
    "    # (\"rankSVM\", get_rankSVM_model(base_model)),\n",
    "    (\"coordAscent\", get_coord_ascent_model(base_model)),\n",
    "    (\"randomForest\", get_random_forest_model(base_model)),\n",
    "    (\"rankNet\", get_ranknet_model(base_model))\n",
    "]\n",
    "\n",
    "fitting_durations = []\n",
    "for model_name, model in models:\n",
    "    print(f\"\\nTraining {model_name}\")\n",
    "    start = datetime.datetime.now()\n",
    "    model.fit(*dataset.get_train())\n",
    "    fitting_durations.append((datetime.datetime.now() - start).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import nDCG, RR, MAP\n",
    "\n",
    "basic_evaluations = pt.Experiment(\n",
    "    [base_model] + [model for _, model in models],\n",
    "    *dataset.get_test(),\n",
    "    names=[\"BM25\"] + [model_name for model_name, _ in models],\n",
    "    eval_metrics=[nDCG @ 10, RR @ 10, MAP],\n",
    ")\n",
    "\n",
    "basic_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness import fairness_evaluation, compute_df\n",
    "\n",
    "baseline_df = compute_df(\n",
    "    base_model, \n",
    "    *dataset.get_test(),\n",
    ")\n",
    "\n",
    "model_dfs = [\n",
    "    compute_df(\n",
    "        model, \n",
    "        *dataset.get_test(),\n",
    "    ) for _, model in models\n",
    "]\n",
    "\n",
    "# print(\"Baseline\")\n",
    "# display(baseline_df)\n",
    "# print(\"Models\")\n",
    "# for model_name, model_df in zip([model_name for model_name, _ in models], model_dfs):\n",
    "#     print(model_name)\n",
    "#     display(model_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_evaluations = [fairness_evaluation(model_df, baseline_df, text_field=dataset.primary_field) for model_df in model_dfs]\n",
    "\n",
    "for model_name, fairness_evaluation in zip([model_name for model_name, _ in models], fairness_evaluations):\n",
    "    print(model_name)\n",
    "    display(fairness_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "import json\n",
    "import os\n",
    "\n",
    "output_dir = os.path.join(\"experiments\", dataset.name + \"_results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the basic evaluations\n",
    "basic_evaluations[\"time\"] = [None] + fitting_durations\n",
    "basic_evaluations.to_csv(os.path.join(output_dir, \"basic_evaluations.csv\"), index=False)\n",
    "\n",
    "#  Combine the fairness evaluations into a single json\n",
    "final_fairness = []\n",
    "for model_name, eval in zip([model_name for model_name, _ in models], fairness_evaluations):\n",
    "    eval[\"model\"] = model_name\n",
    "    final_fairness.append(eval)\n",
    "\n",
    "# Save the fairness evaluations\n",
    "with open(os.path.join(output_dir, \"fairness_evaluations.json\"), \"w\") as f:\n",
    "    json.dump(final_fairness, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_ranknet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
