{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python 3.10 for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: python-terrier==0.12.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: nltk in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: lightgbm in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: fastrank in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: tensorflow==2.11 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: keras in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: LambdaRankNN in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: pandas in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (2.2.3)\n",
      "Requirement already satisfied: more-itertools in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (10.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (2.32.3)\n",
      "Requirement already satisfied: ir-datasets>=0.3.2 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (0.5.10)\n",
      "Requirement already satisfied: wget in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (3.2)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (1.6.1)\n",
      "Requirement already satisfied: deprecated in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (1.2.18)\n",
      "Requirement already satisfied: scipy in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (1.15.2)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (0.3.7)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (0.5.6)\n",
      "Requirement already satisfied: jinja2 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (3.1.6)\n",
      "Requirement already satisfied: statsmodels in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (0.14.4)\n",
      "Requirement already satisfied: dill in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (0.3.9)\n",
      "Requirement already satisfied: joblib in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (1.4.2)\n",
      "Requirement already satisfied: chest in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from python-terrier==0.12.1) (0.2.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow==2.11) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (24.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (0.31.0)\n",
      "Requirement already satisfied: click in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: cffi in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from fastrank) (1.17.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (4.13.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (2.6.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (5.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (6.0.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (4.4.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (3.3.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (0.2.3)\n",
      "Requirement already satisfied: pyarrow>=16.1.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier==0.12.1) (19.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from requests->python-terrier==0.12.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from requests->python-terrier==0.12.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from requests->python-terrier==0.12.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from requests->python-terrier==0.12.1) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tqdm->python-terrier==0.12.1) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from cffi->fastrank) (2.22)\n",
      "Requirement already satisfied: heapdict in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from chest->python-terrier==0.12.1) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from jinja2->python-terrier==0.12.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from pandas->python-terrier==0.12.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from pandas->python-terrier==0.12.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from pandas->python-terrier==0.12.1) (2025.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from statsmodels->python-terrier==0.12.1) (1.0.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow==2.11) (0.45.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier==0.12.1) (2.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (3.1.3)\n",
      "Requirement already satisfied: cbor>=1.0.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier==0.12.1) (1.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2.0\" python-terrier==0.12.1 nltk scikit-learn lightgbm fastrank tensorflow==2.11 keras LambdaRankNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sebim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"irds:nfcorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nfcorpus documents: 100%|██████████| 5371/5371 [00:05<00:00, 1007.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "index = pt.index.IterDictIndexer(\n",
    "    str(Path.cwd()),  # this will be ignored\n",
    "    meta={\n",
    "        \"docno\": 16,\n",
    "        \"title\": 256,\n",
    "        \"abstract\": 65536,\n",
    "        \"url\": 128,\n",
    "    },\n",
    "    type=pt.index.IndexingType.MEMORY,\n",
    ").index(dataset.get_corpus_iter(), fields=[\"title\", \"abstract\", \"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = pt.terrier.FeaturesRetriever(\n",
    "    index,\n",
    "    wmodel=\"BM25\",\n",
    "    features=[\"WMODEL:BM25\", \"WMODEL:PL2\", \"WMODEL:DPH\"],\n",
    "    num_results=100,\n",
    "    metadata=[\"docno\", \"title\", \"abstract\", \"url\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lambdaMART import *\n",
    "from models.lambdaRank import *\n",
    "from models.rankSVM import *\n",
    "from models.coordAscent import *\n",
    "from models.randomForest import *\n",
    "from models.rankNet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lambdaMART\n",
      "21:59:25.735 [main] WARN org.terrier.querying.ApplyTermPipeline -- The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of 'Stopwords,PorterStemmer'. Set a termpipelines control to remove this warning.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 109480, number of used features: 3\n",
      "Training rankNet\n",
      "Epoch 1/10\n",
      "389/389 [==============================] - 1s 1ms/step - loss: 0.0194\n",
      "Epoch 2/10\n",
      "389/389 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 3/10\n",
      "389/389 [==============================] - 1s 2ms/step - loss: 0.0182\n",
      "Epoch 4/10\n",
      "389/389 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 5/10\n",
      "389/389 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 6/10\n",
      "389/389 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 7/10\n",
      "389/389 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 8/10\n",
      "389/389 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 9/10\n",
      "389/389 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 10/10\n",
      "389/389 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "ndcg: 0.9578656125095901\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "models = [\n",
    "    (\"lambdaMART\", get_lambdaMART_model(base_model)),\n",
    "    # (\"lambdaRank\", get_laWmbdaRank_model(base_model)),\n",
    "    # (\"rankSVM\", get_rankSVM_model(base_model)),\n",
    "    # (\"coordAscent\", get_coord_ascent_model(base_model)),\n",
    "    # (\"randomForest\", get_random_forest_model(base_model)),\n",
    "    (\"rankNet\", get_ranknet_model(base_model))\n",
    "]\n",
    "\n",
    "\n",
    "fitting_args = (\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_qrels(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_qrels(),\n",
    ")\n",
    "\n",
    "fitting_durations = []\n",
    "for model_name, model in models:\n",
    "    print(f\"Training {model_name}\")\n",
    "    start = datetime.datetime.now()\n",
    "    model.fit(*fitting_args)\n",
    "    fitting_durations.append((datetime.datetime.now() - start).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRanker was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>RR@10</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.243556</td>\n",
       "      <td>0.451312</td>\n",
       "      <td>0.092002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lambdaMART</td>\n",
       "      <td>0.245393</td>\n",
       "      <td>0.440647</td>\n",
       "      <td>0.095133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rankNet</td>\n",
       "      <td>0.240009</td>\n",
       "      <td>0.438385</td>\n",
       "      <td>0.090380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name   nDCG@10     RR@10        AP\n",
       "0        BM25  0.243556  0.451312  0.092002\n",
       "1  lambdaMART  0.245393  0.440647  0.095133\n",
       "2     rankNet  0.240009  0.438385  0.090380"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier.measures import nDCG, RR, MAP\n",
    "\n",
    "test_datasets = (\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_qrels(),\n",
    ")\n",
    "\n",
    "basic_evaluations = pt.Experiment(\n",
    "    [base_model] + [model for _, model in models],\n",
    "    *test_datasets,\n",
    "    names=[\"BM25\"] + [model_name for model_name, _ in models],\n",
    "    eval_metrics=[nDCG @ 10, RR @ 10, MAP],\n",
    ")\n",
    "\n",
    "basic_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\information_retrieval_ltr_comparison\\.conda\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRanker was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from fairness import fairness_evaluation, compute_df\n",
    "\n",
    "baseline_df = compute_df(\n",
    "    base_model, \n",
    "    *test_datasets,\n",
    ")\n",
    "\n",
    "model_dfs = [\n",
    "    compute_df(\n",
    "        model, \n",
    "        *test_datasets,\n",
    "    ) for _, model in models\n",
    "]\n",
    "\n",
    "# print(\"Baseline\")\n",
    "# display(baseline_df)\n",
    "# print(\"Models\")\n",
    "# for model_name, model_df in zip([model_name for model_name, _ in models], model_dfs):\n",
    "#     print(model_name)\n",
    "#     display(model_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambdaMART\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'InterQuery': {'mean_nDCG': 0.9368,\n",
       "  'std_nDCG': 0.0955,\n",
       "  'range_nDCG': 0.4969,\n",
       "  'fairness_score': 0.898},\n",
       " 'LabelInversionRate': 0.1589,\n",
       " 'IndividualFairnessViolation': 0.0,\n",
       " 'KendallsTauVsBaseline': 0.6951}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rankNet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'InterQuery': {'mean_nDCG': 0.944,\n",
       "  'std_nDCG': 0.091,\n",
       "  'range_nDCG': 0.4187,\n",
       "  'fairness_score': 0.9036},\n",
       " 'LabelInversionRate': 0.1409,\n",
       " 'IndividualFairnessViolation': 0.0,\n",
       " 'KendallsTauVsBaseline': 0.9479}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fairness_evaluations = [fairness_evaluation(model_df, baseline_df) for model_df in model_dfs]\n",
    "\n",
    "for model_name, fairness_evaluation in zip([model_name for model_name, _ in models], fairness_evaluations):\n",
    "    print(model_name)\n",
    "    display(fairness_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "import json\n",
    "import os\n",
    "\n",
    "output_dir = \"BM_25_letor\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the basic evaluations\n",
    "basic_evaluations[\"time\"] = [None] + fitting_durations\n",
    "basic_evaluations.to_csv(os.path.join(output_dir, \"basic_evaluations.csv\"), index=False)\n",
    "\n",
    "#  Combine the fairness evaluations into a single json\n",
    "final_fairness = []\n",
    "for model_name, eval in zip([model_name for model_name, _ in models], fairness_evaluations):\n",
    "    eval[\"model\"] = model_name\n",
    "    final_fairness.append(eval)\n",
    "\n",
    "# Save the fairness evaluations\n",
    "with open(os.path.join(output_dir, \"fairness_evaluation.json\"), \"w\") as f:\n",
    "    json.dump(final_fairness, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_ranknet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
